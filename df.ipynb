{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28236\\2424941275.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0msubject_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mgender\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# Load behavioral summaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mcalming_behav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_behavioral\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calming'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubject_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mvexing_behav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_behavioral\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Vexing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubject_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# Combine sessions — preserves session order (Calming first, then Vexing)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28236\\2424941275.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(session_name, subject_id)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33mf\"\u001b[0m\u001b[1;33mdata/Behavioral_data/\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msession_name\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33mSubject\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m.csv\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Filter out \"Rest\" blocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TrialNumber'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Clean n_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_back'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_back'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wdeha\\miniforge3\\envs\\dsc80\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1575\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1577\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1578\u001b[0m             \u001b[1;33mf\"\u001b[0m\u001b[1;33mThe truth value of a \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m is ambiguous. \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sampling rate\n",
    "sampling_rate = 2000  # Hz\n",
    "\n",
    "# === Function to load and process behavioral data ===\n",
    "def load_behavioral(session_name, subject_id):\n",
    "    # Load CSV\n",
    "    filename = f\"data/Behavioral_data/{session_name}Subject{subject_id}.csv\"\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Filter out \"Rest\" blocks\n",
    "    df = df[df['n_back'] != 'Rest']\n",
    "    \n",
    "    # Clean n_back\n",
    "    df['n_back'] = df['n_back'].str.strip().str.lower()\n",
    "    \n",
    "    # Map to 1_back / 3_back\n",
    "    def map_nback(text):\n",
    "        if 'one back' in text:\n",
    "            return '1_back'\n",
    "        elif 'three back' in text:\n",
    "            return '3_back'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    df['n_back_task'] = df['n_back'].apply(map_nback)\n",
    "    \n",
    "    # Correctness\n",
    "    df['correct'] = df['Response'] == df['Correct_Response']\n",
    "    \n",
    "    # Group by TrialNumber (block) and n_back\n",
    "    trial_summary = df.groupby(['TrialNumber', 'n_back_task']).agg(\n",
    "        accuracy=('correct', 'mean'),\n",
    "        mean_rt=('Response_Time', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Add session label\n",
    "    trial_summary['session'] = session_name.lower()\n",
    "    \n",
    "    return trial_summary\n",
    "\n",
    "# === Main loop over participants ===\n",
    "participants = ['3F', '4F', '6M', '8M', '11F']\n",
    "all_data = []\n",
    "\n",
    "for pid in participants:\n",
    "    subject_num = pid[:-1]\n",
    "    gender = pid[-1]\n",
    "    \n",
    "    # Load behavioral summaries\n",
    "    calming_behav = load_behavioral('Calming', subject_num)\n",
    "    vexing_behav = load_behavioral('Vexing', subject_num)\n",
    "    \n",
    "    # Combine sessions — preserves session order (Calming first, then Vexing)\n",
    "    behavior_df = pd.concat([calming_behav, vexing_behav], ignore_index=True)\n",
    "    \n",
    "    # Load EDA and TRIGGERS_BLOCK (block-level triggers!)\n",
    "    eda_path = f\"data/Biopac_data/EDA/Subject{subject_num}{gender}_EDA.csv\"\n",
    "    triggers_path = f\"data/Biopac_data/Timing/Subject{subject_num}{gender}_Triggers_block.csv\"\n",
    "    \n",
    "    eda_df = pd.read_csv(eda_path, header=None, names=['EDA'])\n",
    "    triggers_df = pd.read_csv(triggers_path)\n",
    "    \n",
    "    print(f\"\\nParticipant {pid} → triggers_df shape: {triggers_df.shape} (should be ~8 rows, wide format)\")\n",
    "\n",
    "    # === Loop through behavioral rows — IN ORIGINAL ORDER ===\n",
    "    for idx, behav_row in behavior_df.sort_values(['session', 'TrialNumber']).iterrows():\n",
    "        session_label = behav_row['session']\n",
    "        nback_label = behav_row['n_back_task']\n",
    "        trial_number = behav_row['TrialNumber']\n",
    "        \n",
    "        found = False\n",
    "        \n",
    "        # Search for first unused matching trigger\n",
    "        for t_idx, t_row in triggers_df.iterrows():\n",
    "            start_col = f\"{session_label}_{nback_label}_start\"\n",
    "            end_col = f\"{session_label}_{nback_label}_end\"\n",
    "            \n",
    "            if pd.notna(t_row[start_col]) and pd.notna(t_row[end_col]):\n",
    "                # Process this EDA slice\n",
    "                start_time = t_row[start_col]\n",
    "                end_time = t_row[end_col]\n",
    "                \n",
    "                start_idx = int(start_time * sampling_rate)\n",
    "                end_idx = int(end_time * sampling_rate)\n",
    "                \n",
    "                eda_slice = eda_df['EDA'].iloc[start_idx:end_idx]\n",
    "                mean_eda = eda_slice.mean()\n",
    "                \n",
    "                # Mark this trigger as used\n",
    "                triggers_df.at[t_idx, start_col] = None\n",
    "                triggers_df.at[t_idx, end_col] = None\n",
    "                \n",
    "                # Append row\n",
    "                all_data.append({\n",
    "                    'participant': pid,\n",
    "                    'trial': len(all_data) + 1,\n",
    "                    'condition': f\"{session_label.capitalize()} {nback_label.replace('_', '-').capitalize()}\",\n",
    "                    'mean_eda': 0 if pd.isna(mean_eda) else mean_eda,\n",
    "                    'accuracy': 0 if pd.isna(behav_row['accuracy']) else behav_row['accuracy'],\n",
    "                    'mean_rt': 0 if pd.isna(behav_row['mean_rt']) else behav_row['mean_rt']\n",
    "                })\n",
    "                \n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"WARNING: No matching trigger found for {session_label} {nback_label} TrialNumber {trial_number}\")\n",
    "\n",
    "    # Participant progress summary\n",
    "    trials_processed = len(all_data) // len(participants)\n",
    "    print(f\"Participant {pid}: {trials_processed} trials processed (expected: 32)\")\n",
    "\n",
    "# Combine all participants into one DataFrame\n",
    "combined_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Optional: save to CSV\n",
    "combined_df.to_csv('df.csv', index=False)\n",
    "\n",
    "# Preview result\n",
    "print(f\"\\nFinal combined_df shape: {combined_df.shape}\")\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Participant 3F: EDA triggers (8, 8), OHb triggers (8, 8)\n",
      "Participant 3F processed 6 trials (expected 32)\n",
      "\n",
      "Participant 4F: EDA triggers (8, 8), OHb triggers (8, 8)\n",
      "Participant 4F processed 12 trials (expected 32)\n",
      "\n",
      "Participant 6M: EDA triggers (8, 8), OHb triggers (8, 8)\n",
      "Participant 6M processed 19 trials (expected 32)\n",
      "\n",
      "Participant 8M: EDA triggers (8, 8), OHb triggers (8, 8)\n",
      "Participant 8M processed 25 trials (expected 32)\n",
      "\n",
      "Participant 11F: EDA triggers (8, 8), OHb triggers (8, 8)\n",
      "Participant 11F processed 32 trials (expected 32)\n",
      "\n",
      "Final combined shape: (160, 6)\n",
      "  participant       condition  mean_eda  mean_ohb  accuracy     mean_rt\n",
      "0          3F  Calming 1-back  7.537035  0.008164  0.863636  564.409091\n",
      "1          3F  Calming 3-back  7.053329  0.003207  0.681818  488.136364\n",
      "2          3F  Calming 3-back  6.492472 -0.000443  0.727273  492.272727\n",
      "3          3F  Calming 1-back  6.135949 -0.000339  0.954545  308.954545\n",
      "4          3F  Calming 3-back  5.944514  0.000300  0.681818  656.863636\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sampling rates\n",
    "EDA_SAMPLING_RATE = 2000      # Hz\n",
    "OHB_SAMPLING_RATE = 7.6294    # Hz\n",
    "\n",
    "# === Function to load and process behavioral data ===\n",
    "def load_behavioral(session_name, subject_id):\n",
    "    filename = f\"data/Behavioral_data/{session_name}Subject{subject_id}.csv\"\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df[df['n_back'] != 'Rest']\n",
    "    df['n_back'] = df['n_back'].str.strip().str.lower()\n",
    "\n",
    "    def map_nback(text):\n",
    "        if 'one back' in text:\n",
    "            return '1_back'\n",
    "        elif 'three back' in text:\n",
    "            return '3_back'\n",
    "        return None\n",
    "    df['n_back_task'] = df['n_back'].apply(map_nback)\n",
    "\n",
    "    df['correct'] = df['Response'] == df['Correct_Response']\n",
    "    summary = (\n",
    "        df.groupby(['TrialNumber', 'n_back_task'])\n",
    "          .agg(accuracy=('correct', 'mean'), mean_rt=('Response_Time', 'mean'))\n",
    "          .reset_index()\n",
    "    )\n",
    "    summary['session'] = session_name.lower()\n",
    "    return summary\n",
    "\n",
    "# === Main processing ===\n",
    "participants = ['3F', '4F', '6M', '8M', '11F']\n",
    "all_data = []\n",
    "\n",
    "for pid in participants:\n",
    "    subj, gender = pid[:-1], pid[-1]\n",
    "    calming = load_behavioral('Calming', subj)\n",
    "    vexing = load_behavioral('Vexing', subj)\n",
    "    behav_df = pd.concat([calming, vexing], ignore_index=True)\n",
    "\n",
    "    # Load EDA data and triggers (first 8 cols)\n",
    "    eda = pd.read_csv(\n",
    "        f\"data/Biopac_data/EDA/Subject{subj}{gender}_EDA.csv\",\n",
    "        header=None, names=['EDA']\n",
    "    )\n",
    "    trig_eda = pd.read_csv(\n",
    "        f\"data/Biopac_data/Timing/Subject{subj}{gender}_Triggers_block.csv\"\n",
    "    ).iloc[:, :8]\n",
    "\n",
    "    # Load OHb raw data as a single series and its triggers (first 8 cols)\n",
    "    raw_ohb = pd.read_csv(\n",
    "        f\"data/fNIRS_data/ohb/Subject{subj}{gender}_ohb.csv\",\n",
    "        header=None\n",
    "    )\n",
    "    # Flatten to 1D and create a Series\n",
    "    ohb = pd.Series(raw_ohb.values.flatten(), name='ohb')\n",
    "    trig_ohb = pd.read_csv(\n",
    "        f\"data/fNIRS_data/Subject{subj}{gender}_Triggers.csv\"\n",
    "    ).iloc[:, :8]\n",
    "\n",
    "    print(f\"\\nParticipant {pid}: EDA triggers {trig_eda.shape}, OHb triggers {trig_ohb.shape}\")\n",
    "\n",
    "    # Iterate through trials in original order\n",
    "    for _, behav in behav_df.sort_values(['session', 'TrialNumber']).iterrows():\n",
    "        sess = behav['session']\n",
    "        task = behav['n_back_task']\n",
    "        start_col = f\"{sess}_{task}_start\"\n",
    "        end_col   = f\"{sess}_{task}_end\"\n",
    "        found = False\n",
    "\n",
    "        # Check each trigger row index\n",
    "        for idx in trig_eda.index:\n",
    "            e_start = trig_eda.at[idx, start_col]\n",
    "            e_end   = trig_eda.at[idx, end_col]\n",
    "            if pd.notna(e_start) and pd.notna(e_end):\n",
    "                # EDA slice and mean\n",
    "                s_e = int(e_start * EDA_SAMPLING_RATE)\n",
    "                e_e = int(e_end   * EDA_SAMPLING_RATE)\n",
    "                mean_eda = eda['EDA'].iloc[s_e:e_e].mean()\n",
    "                trig_eda.at[idx, start_col] = None\n",
    "                trig_eda.at[idx, end_col]   = None\n",
    "\n",
    "                # OHb slice and mean using its own triggers\n",
    "                o_start = trig_ohb.at[idx, start_col]\n",
    "                o_end   = trig_ohb.at[idx, end_col]\n",
    "                s_o = int(o_start * OHB_SAMPLING_RATE)\n",
    "                e_o = int(o_end   * OHB_SAMPLING_RATE)\n",
    "                mean_ohb = ohb.iloc[s_o:e_o].mean()\n",
    "                trig_ohb.at[idx, start_col] = None\n",
    "                trig_ohb.at[idx, end_col]   = None\n",
    "\n",
    "                all_data.append({\n",
    "                    'participant': pid,\n",
    "                    'condition': f\"{sess.capitalize()} {task.replace('_','-').capitalize()}\",\n",
    "                    'mean_eda': mean_eda,\n",
    "                    'mean_ohb': 0 if pd.isna(mean_ohb) else mean_ohb,\n",
    "                    'accuracy': behav['accuracy'],\n",
    "                    'mean_rt': behav['mean_rt']\n",
    "                })\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            print(f\"WARNING: No trigger for {sess} {task} trial {behav['TrialNumber']}\")\n",
    "\n",
    "    processed = len(all_data) // len(participants)\n",
    "    print(f\"Participant {pid} processed {processed} trials (expected 32)\")\n",
    "\n",
    "# Save with full precision\n",
    "df_combined = pd.DataFrame(all_data)\n",
    "df_combined.to_csv('data/df.csv', index=False, float_format='%.18e')\n",
    "\n",
    "print(f\"\\nFinal combined shape: {df_combined.shape}\")\n",
    "print(df_combined.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject_3F...\n",
      "  Processing file 1: Subject_3F_Analysis_1_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n",
      "  Processing file 2: Subject_3F_Analysis_2_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n",
      "  Processing file 3: Subject_3F_Analysis_3_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing file 4: Subject_3F_Analysis_4_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (2428, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n",
      "  Concatenated 4 files for subject_3F\n",
      "Successfully processed subject_3F - 71761 rows\n",
      "Processing subject_4F...\n",
      "  Processing file 1: Subject_4F_Analysis_1_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n",
      "  Processing file 2: Subject_4F_Analysis_2_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n",
      "  Processing file 3: Subject_4F_Analysis_3_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n",
      "  Processing file 4: Subject_4F_Analysis_4_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (18046, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Concatenated 4 files for subject_4F\n",
      "Successfully processed subject_4F - 87379 rows\n",
      "Processing subject_8M...\n",
      "  Processing file 1: Subject_8M_Analysis_1_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n",
      "  Processing file 2: Subject_8M_Analysis_2_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing file 3: Subject_8M_Analysis_3_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (21560, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n",
      "  Concatenated 3 files for subject_8M\n",
      "Successfully processed subject_8M - 67782 rows\n",
      "Processing subject_11F...\n",
      "  Processing file 1: Subject_11F_Analysis_1_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing file 2: Subject_11F_Analysis_2_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n",
      "  Processing file 3: Subject_11F_Analysis_3_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (23111, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "/var/folders/53/_z2krsv122zbsk01mmz12tqc0000gn/T/ipykernel_56990/3614013302.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing file 4: Subject_11F_Analysis_4_video_detailed.txt\n",
      "  Reading with skiprows=5, header=0...\n",
      "  Success! Shape: (47, 10)\n",
      "  Columns: ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
      "  Converting Video Time to seconds...\n",
      "  Concatenated 4 files for subject_11F\n",
      "Successfully processed subject_11F - 69380 rows\n",
      "\n",
      "Final concatenated dataset: 296302 rows, 16 columns\n",
      "Subjects included: ['subject_3F', 'subject_4F', 'subject_8M', 'subject_11F']\n",
      "\n",
      "Columns in final dataset:\n",
      "  - Video Time\n",
      "  - Neutral\n",
      "  - Happy\n",
      "  - Sad\n",
      "  - Angry\n",
      "  - Surprised\n",
      "  - Scared\n",
      "  - Disgusted\n",
      "  - Valence\n",
      "  - Arousal\n",
      "  - Time_seconds\n",
      "  - Time\n",
      "  - Subject\n",
      "  - Video_Number\n",
      "  - Original_Time\n",
      "  - Time_seconds_adjusted\n",
      "\n",
      "Sample of extracted data:\n",
      "     Video Time   Neutral     Happy       Sad     Angry  Surprised    Scared  \\\n",
      "0  00:00:00.000  0.725320  0.001752  0.313713  0.015713   0.005477  0.005976   \n",
      "1  00:00:00.041  0.711566  0.002049  0.318334  0.018173   0.006442  0.012076   \n",
      "2  00:00:00.083  0.699748  0.002356  0.322387  0.020403   0.007285  0.017145   \n",
      "3  00:00:00.125  0.690467  0.002739  0.325771  0.022579   0.008102  0.021219   \n",
      "4  00:00:00.166  0.684551  0.003211  0.326954  0.024626   0.008869  0.025046   \n",
      "5  00:00:00.208  0.677710  0.003701  0.328584  0.026757   0.009498  0.027988   \n",
      "6  00:00:00.250  0.670375  0.004238  0.330563  0.028809   0.010057  0.030168   \n",
      "7  00:00:00.291  0.664808  0.004799  0.330923  0.030766   0.010544  0.032014   \n",
      "8  00:00:00.333  0.656359  0.005295  0.332522  0.032768   0.010944  0.034003   \n",
      "9  00:00:00.375  0.643938  0.005639  0.335751  0.034809   0.011273  0.036266   \n",
      "\n",
      "   Disgusted   Valence   Arousal  Time_seconds          Time     Subject  \\\n",
      "0   0.024165 -0.311961  0.464322         0.000  00:00:00.000  subject_3F   \n",
      "1   0.030354 -0.316285  0.464322         0.041  00:00:00.041  subject_3F   \n",
      "2   0.035767 -0.320031  0.463365         0.083  00:00:00.083  subject_3F   \n",
      "3   0.040750 -0.323032  0.461983         0.125  00:00:00.125  subject_3F   \n",
      "4   0.044649 -0.323743  0.460842         0.166  00:00:00.166  subject_3F   \n",
      "5   0.047882 -0.324883  0.460087         0.208  00:00:00.208  subject_3F   \n",
      "6   0.050972 -0.326325  0.459904         0.250  00:00:00.250  subject_3F   \n",
      "7   0.053852 -0.326125  0.459904         0.291  00:00:00.291  subject_3F   \n",
      "8   0.056577 -0.327227  0.472792         0.333  00:00:00.333  subject_3F   \n",
      "9   0.059190 -0.330112  0.489414         0.375  00:00:00.375  subject_3F   \n",
      "\n",
      "   Video_Number Original_Time  Time_seconds_adjusted  \n",
      "0             1  00:00:00.000                  0.000  \n",
      "1             1  00:00:00.041                  0.041  \n",
      "2             1  00:00:00.083                  0.083  \n",
      "3             1  00:00:00.125                  0.125  \n",
      "4             1  00:00:00.166                  0.166  \n",
      "5             1  00:00:00.208                  0.208  \n",
      "6             1  00:00:00.250                  0.250  \n",
      "7             1  00:00:00.291                  0.291  \n",
      "8             1  00:00:00.333                  0.333  \n",
      "9             1  00:00:00.375                  0.375  \n",
      "\n",
      "Columns in the dataset:\n",
      "  - Video Time\n",
      "  - Neutral\n",
      "  - Happy\n",
      "  - Sad\n",
      "  - Angry\n",
      "  - Surprised\n",
      "  - Scared\n",
      "  - Disgusted\n",
      "  - Valence\n",
      "  - Arousal\n",
      "  - Time_seconds\n",
      "  - Time\n",
      "  - Subject\n",
      "  - Video_Number\n",
      "  - Original_Time\n",
      "  - Time_seconds_adjusted\n",
      "\n",
      "Data summary by subject:\n",
      "                         Time_seconds_adjusted                \n",
      "                                           min      max  count\n",
      "Subject     Video_Number                                      \n",
      "subject_11F 1                             0.00   963.88  23111\n",
      "            2                           963.88  1927.76  23111\n",
      "            3                          1927.76  2891.64  23111\n",
      "            4                          2891.64  2893.56     47\n",
      "subject_3F  1                             0.00   963.88  23111\n",
      "            2                           963.88  1927.76  23111\n",
      "            3                          1927.76  2891.64  23111\n",
      "            4                          2891.64  2992.86   2428\n",
      "subject_4F  1                             0.00   963.88  23111\n",
      "            2                           963.88  1927.76  23111\n",
      "            3                          1927.76  2891.64  23111\n",
      "            4                          2891.64  3644.26  18046\n",
      "subject_8M  1                             0.00   963.88  23111\n",
      "            2                           963.88  1927.76  23111\n",
      "            3                          1927.76  2826.95  21560\n",
      "\n",
      "Checking for failed facial recognition data:\n",
      "  Neutral: 106329/296302 (35.9%) failed readings\n",
      "  Happy: 106329/296302 (35.9%) failed readings\n",
      "  Sad: 106329/296302 (35.9%) failed readings\n",
      "  Angry: 106329/296302 (35.9%) failed readings\n",
      "  Surprised: 106329/296302 (35.9%) failed readings\n",
      "  Scared: 106329/296302 (35.9%) failed readings\n",
      "  Disgusted: 106329/296302 (35.9%) failed readings\n",
      "\n",
      "Data saved to concatenated_face_reader_data.csv\n",
      "\n",
      "Sample emotion data (first 5 rows with valid readings):\n",
      "     Video Time   Neutral     Happy       Sad     Angry  Surprised    Scared  \\\n",
      "0  00:00:00.000  0.725320  0.001752  0.313713  0.015713   0.005477  0.005976   \n",
      "1  00:00:00.041  0.711566  0.002049  0.318334  0.018173   0.006442  0.012076   \n",
      "2  00:00:00.083  0.699748  0.002356  0.322387  0.020403   0.007285  0.017145   \n",
      "3  00:00:00.125  0.690467  0.002739  0.325771  0.022579   0.008102  0.021219   \n",
      "4  00:00:00.166  0.684551  0.003211  0.326954  0.024626   0.008869  0.025046   \n",
      "\n",
      "   Disgusted   Valence   Arousal  \n",
      "0   0.024165 -0.311961  0.464322  \n",
      "1   0.030354 -0.316285  0.464322  \n",
      "2   0.035767 -0.320031  0.463365  \n",
      "3   0.040750 -0.323032  0.461983  \n",
      "4   0.044649 -0.323743  0.460842  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "def parse_time_to_seconds(time_str):\n",
    "    \"\"\"Convert time string (HH:MM:SS or HH:MM:SS.mmm) to seconds\"\"\"\n",
    "    try:\n",
    "        # Handle microseconds if present\n",
    "        if '.' in time_str:\n",
    "            time_part, microsec_part = time_str.split('.')\n",
    "            microseconds = int(microsec_part.ljust(6, '0')[:6])  # Pad or truncate to 6 digits\n",
    "        else:\n",
    "            time_part = time_str\n",
    "            microseconds = 0\n",
    "        \n",
    "        # Parse time components\n",
    "        time_parts = time_part.split(':')\n",
    "        hours = int(time_parts[0])\n",
    "        minutes = int(time_parts[1])\n",
    "        seconds = int(time_parts[2])\n",
    "        \n",
    "        total_seconds = hours * 3600 + minutes * 60 + seconds + microseconds / 1000000\n",
    "        return total_seconds\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing time '{time_str}': {e}\")\n",
    "        return 0\n",
    "\n",
    "def inspect_file_structure(filepath, num_lines=10):\n",
    "    \"\"\"Inspect the structure of a file to understand its format\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            lines = [f.readline().strip() for _ in range(num_lines)]\n",
    "        \n",
    "        print(f\"\\nInspecting {os.path.basename(filepath)}:\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if line:  # Only print non-empty lines\n",
    "                print(f\"Line {i+1}: {line}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        return lines\n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting {filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "def read_detailed_file(filepath):\n",
    "    \"\"\"Read a detailed.txt file and return DataFrame with robust parsing\"\"\"\n",
    "    try:\n",
    "        # Based on the file structure analysis, we know:\n",
    "        # Line 1: \"Video analysis detailed log\"\n",
    "        # Line 2: (empty)\n",
    "        # Line 3: \"Face Model: General\"\n",
    "        # Line 4: \"Frame rate: 23.98\"\n",
    "        # Line 5: (empty)\n",
    "        # Line 6: Header row with column names\n",
    "        # Line 7+: Data rows\n",
    "        \n",
    "        # So we need to skip the first 5 rows and use row 6 as header (index 5, or skiprows=5)\n",
    "        print(f\"  Reading with skiprows=5, header=0...\")\n",
    "        df = pd.read_csv(filepath, sep='\\t', skiprows=5, header=0)\n",
    "        \n",
    "        if not df.empty and len(df.columns) > 1:\n",
    "            print(f\"  Success! Shape: {df.shape}\")\n",
    "            print(f\"  Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Convert Video Time column to seconds\n",
    "            if 'Video Time' in df.columns:\n",
    "                print(f\"  Converting Video Time to seconds...\")\n",
    "                df['Time_seconds'] = df['Video Time'].apply(parse_time_to_seconds)\n",
    "                df['Time'] = df['Video Time']  # Keep original for reference\n",
    "            \n",
    "            # Handle FIT_FAILED and FIND_FAILED values\n",
    "            # Replace these with NaN for numeric columns\n",
    "            for col in df.columns:\n",
    "                if col not in ['Video Time', 'Time']:\n",
    "                    df[col] = df[col].replace(['FIT_FAILED', 'FIND_FAILED'], pd.NA)\n",
    "                    # Try to convert to numeric, keeping non-numeric as is\n",
    "                    df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(f\"  Failed to read properly - shape: {df.shape}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading {filepath}: {e}\")\n",
    "        # Fallback to the original robust approach\n",
    "        return read_detailed_file_fallback(filepath)\n",
    "\n",
    "def read_detailed_file_fallback(filepath):\n",
    "    \"\"\"Fallback method for reading detailed files\"\"\"\n",
    "    try:\n",
    "        # First, inspect the file structure\n",
    "        lines = inspect_file_structure(filepath)\n",
    "        \n",
    "        # Try different parsing approaches\n",
    "        approaches = [\n",
    "            # Skip initial lines that might be metadata\n",
    "            {'sep': '\\t', 'skiprows': 5, 'header': 0},\n",
    "            {'sep': '\\t', 'skiprows': 4, 'header': 0},\n",
    "            {'sep': '\\t', 'skiprows': 6, 'header': 0},\n",
    "            {'sep': '\\t', 'skiprows': 3, 'header': 0},\n",
    "            # Standard tab-separated with different header positions\n",
    "            {'sep': '\\t', 'header': 5},\n",
    "            {'sep': '\\t', 'header': 4},\n",
    "            {'sep': '\\t', 'header': 6},\n",
    "        ]\n",
    "        \n",
    "        for i, params in enumerate(approaches):\n",
    "            try:\n",
    "                print(f\"  Trying fallback approach {i+1}: {params}\")\n",
    "                df = pd.read_csv(filepath, **params)\n",
    "                \n",
    "                if not df.empty and len(df.columns) > 5:  # Expect at least 6 columns\n",
    "                    print(f\"  Success! Shape: {df.shape}\")\n",
    "                    print(f\"  Columns: {list(df.columns)}\")\n",
    "                    \n",
    "                    # Convert Time column to seconds if it exists\n",
    "                    time_columns = [col for col in df.columns if 'time' in col.lower()]\n",
    "                    if time_columns:\n",
    "                        time_col = time_columns[0]\n",
    "                        print(f\"  Found time column: {time_col}\")\n",
    "                        df['Time_seconds'] = df[time_col].apply(parse_time_to_seconds)\n",
    "                        df['Time'] = df[time_col]\n",
    "                    \n",
    "                    return df\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Fallback approach {i+1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"  All fallback approaches failed.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fallback error reading {filepath}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def manual_parse_file(filepath):\n",
    "    \"\"\"Manually parse file when standard methods fail\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Find the actual data start (look for lines with consistent field counts)\n",
    "        data_lines = []\n",
    "        header_line = None\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Try different separators\n",
    "            for sep in ['\\t', ',', ' ']:\n",
    "                fields = line.split(sep)\n",
    "                if len(fields) > 5:  # Assuming at least 5 fields for meaningful data\n",
    "                    if header_line is None:\n",
    "                        # This might be the header\n",
    "                        header_line = fields\n",
    "                        header_idx = i\n",
    "                        break\n",
    "                    elif len(fields) == len(header_line):\n",
    "                        # This is a data line with matching field count\n",
    "                        data_lines.append(fields)\n",
    "                        break\n",
    "        \n",
    "        if header_line and data_lines:\n",
    "            df = pd.DataFrame(data_lines, columns=header_line)\n",
    "            print(f\"  Manual parsing successful! Shape: {df.shape}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"  Manual parsing failed - could not identify data structure\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Manual parsing error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_video_duration(df):\n",
    "    \"\"\"Get the duration of a video from its DataFrame\"\"\"\n",
    "    if df.empty or 'Time_seconds' not in df.columns:\n",
    "        return 0\n",
    "    return df['Time_seconds'].max()\n",
    "\n",
    "def process_subject_data(subject_folder):\n",
    "    \"\"\"Process all video files for a single subject\"\"\"\n",
    "    subject_name = os.path.basename(subject_folder)\n",
    "    print(f\"Processing {subject_name}...\")\n",
    "    \n",
    "    # Find all detailed.txt files\n",
    "    detailed_files = glob.glob(os.path.join(subject_folder, \"*detailed.txt\"))\n",
    "    detailed_files.sort()  # Ensure proper order\n",
    "    \n",
    "    if not detailed_files:\n",
    "        print(f\"No detailed.txt files found for {subject_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    all_dataframes = []\n",
    "    cumulative_time = 0\n",
    "    \n",
    "    for i, filepath in enumerate(detailed_files):\n",
    "        print(f\"  Processing file {i+1}: {os.path.basename(filepath)}\")\n",
    "        \n",
    "        # Read the file\n",
    "        df = read_detailed_file(filepath)\n",
    "        \n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Add subject identifier\n",
    "        df['Subject'] = subject_name\n",
    "        df['Video_Number'] = i + 1\n",
    "        df['Original_Time'] = df['Time'] if 'Time' in df.columns else None\n",
    "        \n",
    "        # Adjust time for sequential videos (videos 2, 3, 4, etc.)\n",
    "        if i > 0 and 'Time_seconds' in df.columns:\n",
    "            df['Time_seconds_adjusted'] = df['Time_seconds'] + cumulative_time\n",
    "        else:\n",
    "            df['Time_seconds_adjusted'] = df['Time_seconds'] if 'Time_seconds' in df.columns else 0\n",
    "        \n",
    "        # Update cumulative time for next video\n",
    "        if 'Time_seconds' in df.columns:\n",
    "            video_duration = get_video_duration(df)\n",
    "            cumulative_time += video_duration\n",
    "        \n",
    "        all_dataframes.append(df)\n",
    "    \n",
    "    if all_dataframes:\n",
    "        concatenated_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        print(f\"  Concatenated {len(all_dataframes)} files for {subject_name}\")\n",
    "        return concatenated_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def extract_all_face_reader_data(base_path=\"data/Face_reader_data\"):\n",
    "    \"\"\"Extract and concatenate all Face Reader data\"\"\"\n",
    "    \n",
    "    # Define subjects and their expected video counts\n",
    "    subjects_info = {\n",
    "        'subject_3F': 4,\n",
    "        'subject_4F': 4,\n",
    "        'subject_8M': 3,\n",
    "        'subject_11F': 4\n",
    "    }\n",
    "    \n",
    "    all_subjects_data = []\n",
    "    \n",
    "    for subject_name, expected_videos in subjects_info.items():\n",
    "        subject_folder = os.path.join(base_path, subject_name)\n",
    "        \n",
    "        if not os.path.exists(subject_folder):\n",
    "            print(f\"Warning: Folder {subject_folder} not found\")\n",
    "            continue\n",
    "        \n",
    "        # Process subject data\n",
    "        subject_df = process_subject_data(subject_folder)\n",
    "        \n",
    "        if not subject_df.empty:\n",
    "            all_subjects_data.append(subject_df)\n",
    "            print(f\"Successfully processed {subject_name} - {len(subject_df)} rows\")\n",
    "        else:\n",
    "            print(f\"No data extracted for {subject_name}\")\n",
    "    \n",
    "    # Concatenate all subjects\n",
    "    if all_subjects_data:\n",
    "        final_df = pd.concat(all_subjects_data, ignore_index=True)\n",
    "        print(f\"\\nFinal concatenated dataset: {len(final_df)} rows, {len(final_df.columns)} columns\")\n",
    "        print(f\"Subjects included: {final_df['Subject'].unique().tolist()}\")\n",
    "        \n",
    "        # Display column information\n",
    "        print(f\"\\nColumns in final dataset:\")\n",
    "        for col in final_df.columns:\n",
    "            print(f\"  - {col}\")\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No data was successfully extracted from any subject\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Additional utility functions for data analysis\n",
    "\n",
    "def get_onset_time(subject_folder):\n",
    "    \"\"\"Get the onset time of the first video for a subject (if available)\"\"\"\n",
    "    # Look for onset information in state or detailed files\n",
    "    onset_files = glob.glob(os.path.join(subject_folder, \"*1_video*\"))\n",
    "    \n",
    "    # This is a placeholder - you may need to modify based on actual file format\n",
    "    # that contains the unix timestamp onset information\n",
    "    return None\n",
    "\n",
    "def align_with_biopac_time(df, subject_onset_times=None):\n",
    "    \"\"\"Align Face Reader data with Biopac data using onset times\"\"\"\n",
    "    if subject_onset_times is None:\n",
    "        print(\"No onset times provided - returning data with relative timestamps\")\n",
    "        return df\n",
    "    \n",
    "    # Add absolute timestamps based on onset times\n",
    "    for subject in df['Subject'].unique():\n",
    "        if subject in subject_onset_times:\n",
    "            mask = df['Subject'] == subject\n",
    "            onset_time = subject_onset_times[subject]\n",
    "            # Convert relative time to absolute time\n",
    "            df.loc[mask, 'Absolute_Time'] = onset_time + df.loc[mask, 'Time_seconds_adjusted']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def diagnose_files(base_path=\"data/Face_reader_data\"):\n",
    "    \"\"\"Diagnose file structures to understand the format\"\"\"\n",
    "    subjects_info = {\n",
    "        'subject_3F': 4,\n",
    "        'subject_4F': 4,\n",
    "        'subject_8M': 3,\n",
    "        'subject_11F': 4\n",
    "    }\n",
    "    \n",
    "    print(\"=== FILE STRUCTURE DIAGNOSIS ===\")\n",
    "    \n",
    "    for subject_name in subjects_info.keys():\n",
    "        subject_folder = os.path.join(base_path, subject_name)\n",
    "        \n",
    "        if not os.path.exists(subject_folder):\n",
    "            print(f\"Folder {subject_folder} not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- {subject_name} ---\")\n",
    "        detailed_files = glob.glob(os.path.join(subject_folder, \"*detailed.txt\"))\n",
    "        detailed_files.sort()\n",
    "        \n",
    "        if detailed_files:\n",
    "            # Just inspect the first file for each subject\n",
    "            inspect_file_structure(detailed_files[0], num_lines=15)\n",
    "        else:\n",
    "            print(f\"No detailed.txt files found in {subject_folder}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Extract all data\n",
    "    face_reader_df = extract_all_face_reader_data()\n",
    "    \n",
    "    # Display sample of the data\n",
    "    if not face_reader_df.empty:\n",
    "        print(\"\\nSample of extracted data:\")\n",
    "        print(face_reader_df.head(10))\n",
    "        \n",
    "        print(f\"\\nColumns in the dataset:\")\n",
    "        for col in face_reader_df.columns:\n",
    "            print(f\"  - {col}\")\n",
    "        \n",
    "        print(\"\\nData summary by subject:\")\n",
    "        if 'Time_seconds_adjusted' in face_reader_df.columns:\n",
    "            summary = face_reader_df.groupby(['Subject', 'Video_Number']).agg({\n",
    "                'Time_seconds_adjusted': ['min', 'max', 'count']\n",
    "            }).round(2)\n",
    "            print(summary)\n",
    "        \n",
    "        # Check for failed readings\n",
    "        print(\"\\nChecking for failed facial recognition data:\")\n",
    "        for col in ['Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted']:\n",
    "            if col in face_reader_df.columns:\n",
    "                failed_count = face_reader_df[col].isna().sum()\n",
    "                total_count = len(face_reader_df)\n",
    "                print(f\"  {col}: {failed_count}/{total_count} ({failed_count/total_count*100:.1f}%) failed readings\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_filename = \"concatenated_face_reader_data.csv\"\n",
    "        face_reader_df.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nData saved to {output_filename}\")\n",
    "        \n",
    "        # Show some actual emotion data\n",
    "        print(\"\\nSample emotion data (first 5 rows with valid readings):\")\n",
    "        emotion_cols = ['Video Time', 'Neutral', 'Happy', 'Sad', 'Angry', 'Surprised', 'Scared', 'Disgusted', 'Valence', 'Arousal']\n",
    "        available_emotion_cols = [col for col in emotion_cols if col in face_reader_df.columns]\n",
    "        if available_emotion_cols:\n",
    "            # Find first few rows without NaN values\n",
    "            valid_data = face_reader_df.dropna(subset=[col for col in available_emotion_cols if col != 'Video Time'])\n",
    "            if not valid_data.empty:\n",
    "                print(valid_data[available_emotion_cols].head())\n",
    "            else:\n",
    "                print(\"No rows found without failed readings\")\n",
    "    else:\n",
    "        print(\"\\nNo data was extracted. Please check the file format.\")\n",
    "    \n",
    "    # Example of how to use the alignment function (you'll need to provide onset times)\n",
    "    # onset_times = {\n",
    "    #     'subject_3F': 1640995200,  # Example unix timestamp\n",
    "    #     'subject_4F': 1640995300,\n",
    "    #     'subject_8M': 1640995400,\n",
    "    #     'subject_11F': 1640995500\n",
    "    # }\n",
    "    # aligned_df = align_with_biopac_time(face_reader_df, onset_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video Time</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Happy</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Angry</th>\n",
       "      <th>Surprised</th>\n",
       "      <th>Scared</th>\n",
       "      <th>Disgusted</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Time_seconds</th>\n",
       "      <th>Time</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Video_Number</th>\n",
       "      <th>Original_Time</th>\n",
       "      <th>Time_seconds_adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00.000</td>\n",
       "      <td>0.725320</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.313713</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>0.024165</td>\n",
       "      <td>-0.311961</td>\n",
       "      <td>0.464322</td>\n",
       "      <td>0.000</td>\n",
       "      <td>00:00:00.000</td>\n",
       "      <td>subject_3F</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:00:00.041</td>\n",
       "      <td>0.711566</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.318334</td>\n",
       "      <td>0.018173</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.012076</td>\n",
       "      <td>0.030354</td>\n",
       "      <td>-0.316285</td>\n",
       "      <td>0.464322</td>\n",
       "      <td>0.041</td>\n",
       "      <td>00:00:00.041</td>\n",
       "      <td>subject_3F</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00.041</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:00:00.083</td>\n",
       "      <td>0.699748</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.322387</td>\n",
       "      <td>0.020403</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>0.017145</td>\n",
       "      <td>0.035767</td>\n",
       "      <td>-0.320031</td>\n",
       "      <td>0.463365</td>\n",
       "      <td>0.083</td>\n",
       "      <td>00:00:00.083</td>\n",
       "      <td>subject_3F</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00.083</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:00:00.125</td>\n",
       "      <td>0.690467</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.325771</td>\n",
       "      <td>0.022579</td>\n",
       "      <td>0.008102</td>\n",
       "      <td>0.021219</td>\n",
       "      <td>0.040750</td>\n",
       "      <td>-0.323032</td>\n",
       "      <td>0.461983</td>\n",
       "      <td>0.125</td>\n",
       "      <td>00:00:00.125</td>\n",
       "      <td>subject_3F</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00.125</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:00:00.166</td>\n",
       "      <td>0.684551</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>0.326954</td>\n",
       "      <td>0.024626</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>0.025046</td>\n",
       "      <td>0.044649</td>\n",
       "      <td>-0.323743</td>\n",
       "      <td>0.460842</td>\n",
       "      <td>0.166</td>\n",
       "      <td>00:00:00.166</td>\n",
       "      <td>subject_3F</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00.166</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296297</th>\n",
       "      <td>00:00:01.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.751</td>\n",
       "      <td>00:00:01.751</td>\n",
       "      <td>subject_11F</td>\n",
       "      <td>4</td>\n",
       "      <td>00:00:01.751</td>\n",
       "      <td>2893.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296298</th>\n",
       "      <td>00:00:01.793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.793</td>\n",
       "      <td>00:00:01.793</td>\n",
       "      <td>subject_11F</td>\n",
       "      <td>4</td>\n",
       "      <td>00:00:01.793</td>\n",
       "      <td>2893.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296299</th>\n",
       "      <td>00:00:01.835</td>\n",
       "      <td>0.547326</td>\n",
       "      <td>0.227016</td>\n",
       "      <td>0.091208</td>\n",
       "      <td>0.065439</td>\n",
       "      <td>0.176164</td>\n",
       "      <td>0.057305</td>\n",
       "      <td>0.134598</td>\n",
       "      <td>0.092418</td>\n",
       "      <td>0.560882</td>\n",
       "      <td>1.835</td>\n",
       "      <td>00:00:01.835</td>\n",
       "      <td>subject_11F</td>\n",
       "      <td>4</td>\n",
       "      <td>00:00:01.835</td>\n",
       "      <td>2893.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296300</th>\n",
       "      <td>00:00:01.876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.876</td>\n",
       "      <td>00:00:01.876</td>\n",
       "      <td>subject_11F</td>\n",
       "      <td>4</td>\n",
       "      <td>00:00:01.876</td>\n",
       "      <td>2893.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296301</th>\n",
       "      <td>00:00:01.918</td>\n",
       "      <td>0.542648</td>\n",
       "      <td>0.216478</td>\n",
       "      <td>0.107577</td>\n",
       "      <td>0.074805</td>\n",
       "      <td>0.163981</td>\n",
       "      <td>0.071827</td>\n",
       "      <td>0.132593</td>\n",
       "      <td>0.083885</td>\n",
       "      <td>0.552690</td>\n",
       "      <td>1.918</td>\n",
       "      <td>00:00:01.918</td>\n",
       "      <td>subject_11F</td>\n",
       "      <td>4</td>\n",
       "      <td>00:00:01.918</td>\n",
       "      <td>2893.555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296302 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Video Time   Neutral     Happy       Sad     Angry  Surprised  \\\n",
       "0       00:00:00.000  0.725320  0.001752  0.313713  0.015713   0.005477   \n",
       "1       00:00:00.041  0.711566  0.002049  0.318334  0.018173   0.006442   \n",
       "2       00:00:00.083  0.699748  0.002356  0.322387  0.020403   0.007285   \n",
       "3       00:00:00.125  0.690467  0.002739  0.325771  0.022579   0.008102   \n",
       "4       00:00:00.166  0.684551  0.003211  0.326954  0.024626   0.008869   \n",
       "...              ...       ...       ...       ...       ...        ...   \n",
       "296297  00:00:01.751       NaN       NaN       NaN       NaN        NaN   \n",
       "296298  00:00:01.793       NaN       NaN       NaN       NaN        NaN   \n",
       "296299  00:00:01.835  0.547326  0.227016  0.091208  0.065439   0.176164   \n",
       "296300  00:00:01.876       NaN       NaN       NaN       NaN        NaN   \n",
       "296301  00:00:01.918  0.542648  0.216478  0.107577  0.074805   0.163981   \n",
       "\n",
       "          Scared  Disgusted   Valence   Arousal  Time_seconds          Time  \\\n",
       "0       0.005976   0.024165 -0.311961  0.464322         0.000  00:00:00.000   \n",
       "1       0.012076   0.030354 -0.316285  0.464322         0.041  00:00:00.041   \n",
       "2       0.017145   0.035767 -0.320031  0.463365         0.083  00:00:00.083   \n",
       "3       0.021219   0.040750 -0.323032  0.461983         0.125  00:00:00.125   \n",
       "4       0.025046   0.044649 -0.323743  0.460842         0.166  00:00:00.166   \n",
       "...          ...        ...       ...       ...           ...           ...   \n",
       "296297       NaN        NaN       NaN       NaN         1.751  00:00:01.751   \n",
       "296298       NaN        NaN       NaN       NaN         1.793  00:00:01.793   \n",
       "296299  0.057305   0.134598  0.092418  0.560882         1.835  00:00:01.835   \n",
       "296300       NaN        NaN       NaN       NaN         1.876  00:00:01.876   \n",
       "296301  0.071827   0.132593  0.083885  0.552690         1.918  00:00:01.918   \n",
       "\n",
       "            Subject  Video_Number Original_Time  Time_seconds_adjusted  \n",
       "0        subject_3F             1  00:00:00.000                  0.000  \n",
       "1        subject_3F             1  00:00:00.041                  0.041  \n",
       "2        subject_3F             1  00:00:00.083                  0.083  \n",
       "3        subject_3F             1  00:00:00.125                  0.125  \n",
       "4        subject_3F             1  00:00:00.166                  0.166  \n",
       "...             ...           ...           ...                    ...  \n",
       "296297  subject_11F             4  00:00:01.751               2893.388  \n",
       "296298  subject_11F             4  00:00:01.793               2893.430  \n",
       "296299  subject_11F             4  00:00:01.835               2893.472  \n",
       "296300  subject_11F             4  00:00:01.876               2893.513  \n",
       "296301  subject_11F             4  00:00:01.918               2893.555  \n",
       "\n",
       "[296302 rows x 16 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_reader = pd.read_csv('concatenated_face_reader_data.csv')\n",
    "face_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
